# QuantDevProjectSpring2023

Supervised Learning: Algorithms that learn from labeled training data to make predictions or classifications.
a. Linear Regression: Predicts the relationship between input and output variables.
b. Logistic Regression: Classifies input data into two or more classes.
c. Support Vector Machines (SVM): Maximizes the margin between different classes in the data.
d. Decision Trees: Builds a tree-like structure to make decisions based on input features.
e. Random Forest: An ensemble method that combines multiple decision trees to make more accurate predictions.
f. Gradient Boosting Machines (GBM): Builds multiple weak learners sequentially to minimize the loss function.

Unsupervised Learning: Algorithms that identify patterns and relationships in unlabeled data.
a. Clustering Algorithms: Group similar data points together. Examples include K-means, DBSCAN, and hierarchical clustering.
b. Dimensionality Reduction: Reduces the number of input features while preserving important information. Examples include Principal Component Analysis (PCA) and t-Distributed Stochastic Neighbor Embedding (t-SNE).

Reinforcement Learning: Algorithms that learn to make decisions by interacting with an environment, receiving feedback in the form of rewards or penalties.
a. Q-Learning: Learns a Q-function that estimates the expected future rewards for each action taken in a given state.
b. Deep Q-Networks (DQN): Combines Q-learning with deep neural networks for more complex environments.
c. Policy Gradient Methods: Learns a policy that maps states to actions to maximize cumulative rewards.
d. Actor-Critic Methods: Combines the concepts of policy gradients and Q-learning for improved learning and stability.

Deep Learning: Uses neural networks with multiple layers to learn complex patterns in large datasets.
a. Convolutional Neural Networks (CNN): Specialized for processing grid-like data, such as time-series or image data.
b. Recurrent Neural Networks (RNN): Processes sequences of data, such as time-series data, by maintaining hidden states over time. Variants include Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU).
c. Autoencoders: Unsupervised learning models that compress and reconstruct input data, often used for dimensionality reduction or anomaly detection.
